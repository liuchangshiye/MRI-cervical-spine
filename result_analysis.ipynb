{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad15879-77c9-4d03-975f-616fae3e79ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import pydicom\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90f56d-3fb0-4f50-9c9d-736e9a1b1660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify the following two path to your local path\n",
    "img_id_map_file = 'path/to/image_id_map.json'   # a dictionary file with the format {image_name: image_id, ...}\n",
    "cat_id_map_file = 'path/to/category_id_map.json'   # a dictionary file with the format {label_name: label_id, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93649a07-a4af-498b-9101-65c15f7d4b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_img_id_map():\n",
    "    with open(img_id_map_file) as fp:\n",
    "        return json.load(fp)\n",
    "\n",
    "def load_cat_id_map():\n",
    "     with open(cat_id_map_file) as fp:\n",
    "        return json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adaba8-7a04-4e7b-b9b9-98b9648f80e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_id_map = load_img_id_map()\n",
    "cat_id_map = load_cat_id_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198620d-4add-4b3b-aab1-eca1e5ac75d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_img_map = {v: k for k, v in img_id_map.items()}\n",
    "id_cat_map = {v: k for k, v in cat_id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd9e6e-1dba-4b01-8e9c-08dabbab9ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_box_max(results, score_filter=0.5, ca_percent=0.9):\n",
    "    boxes = results['boxes']\n",
    "    labels = results['labels']\n",
    "    scores = results['scores']\n",
    "    # filter\n",
    "    bls = [(results['boxes'][i], results['labels'][i], results['scores'][i])\n",
    "           for i, score in enumerate(results['scores']) if score > score_filter]\n",
    "    if len(bls) == 0:\n",
    "        return [], [], []\n",
    "    # merge box\n",
    "    # If the area overlaps by more than 80%, the label is the one with the higher score, and the bbox is the union of the two.\n",
    "    new_bls = [bls[0]]\n",
    "    for bbox, label, score in bls[1:]:\n",
    "        is_max_cover = False\n",
    "        for i, (nbox, nlabel, nscore) in enumerate(new_bls):\n",
    "            bx1, by1, bx2, by2 = bbox\n",
    "            nx1, ny1, nx2, ny2 = nbox\n",
    "            cx1, cy1, cx2, cy2 = max(bx1, nx1), max(by1, ny1), min(bx2, nx2), min(by2, ny2)\n",
    "            if cx1 <= cx2 and cy1 <= cy2: # confirm overlap\n",
    "                area1 = (bx2-bx1) * (by2-by1)\n",
    "                area2 = (nx2-nx1) * (ny2-ny1)\n",
    "                areac = (cx2-cx1) * (cy2-cy1)\n",
    "                is_max_cover = areac/area1>ca_percent or areac/area2>ca_percent # if overlap meets criteria\n",
    "            if is_max_cover:\n",
    "                dc_box = (min(bx1, nx1), min(by1, ny1), max(bx2, nx2), max(by2, ny2))\n",
    "                dc_label = label if score>nscore else nlabel\n",
    "                dc_score = max(score, nscore)\n",
    "                new_bls[i] = (dc_box, dc_label, dc_score)\n",
    "                break\n",
    "        if not is_max_cover: # No overlap or too little overlap\n",
    "            new_bls.append((bbox, label, score))\n",
    "                \n",
    "    n_boxes, n_labels, n_scores = list(zip(*new_bls))\n",
    "    return n_labels, n_boxes, n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c7ac5-cff7-4b1e-95dc-f60c060e17da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_ground_truth_test(img_id, test_dataset='int-test-inference', dtype='T2_Ax'):\n",
    "    with open('annotations/%s_%s.json' % (test_dataset, dtype)) as fp:\n",
    "        ans = json.load(fp)\n",
    "    return [an for an in ans['annotations'] if an['image_id']==img_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868ef32-2a35-4a09-befa-466330066a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_res_and_ground_truth(img_id, test_epoch, results, ground_truth, score_filter=0.5, ca_percent=0.5):\n",
    "    # ca_percent: cross area percentage\n",
    "    labels, boxes, scores = merge_box_max(results, score_filter)\n",
    "    for merge_ca_percent in [0.8, 0.7, 0.6, 0.5]:\n",
    "        if len(labels)>3:\n",
    "            labels, boxes, scores = merge_box_max({'boxes': boxes, 'labels': labels, 'scores': scores}, score_filter, merge_ca_percent)\n",
    "    \n",
    "    label_pair = []\n",
    "    paired_predicted = set()\n",
    "    for i, an in enumerate(ground_truth):\n",
    "        tbox, tlabel = an['bbox'], an['category_id']\n",
    "        tx1, ty1, tx2, ty2 = tbox[0], tbox[1], tbox[0]+tbox[2], tbox[1]+tbox[3]\n",
    "        paired_index = []\n",
    "        for j, box in enumerate(boxes):\n",
    "            nx1, ny1, nx2, ny2 = box\n",
    "            cx1, cy1, cx2, cy2 = max(tx1, nx1), max(ty1, ny1), min(tx2, nx2), min(ty2, ny2)\n",
    "            if cx1 <= cx2 and cy1 <= cy2: # area overlap\n",
    "                areat = (tx2-tx1) * (ty2-ty1)\n",
    "                arean = (nx2-nx1) * (ny2-ny1)\n",
    "                areac = (cx2-cx1) * (cy2-cy1)\n",
    "                if areac/areat>ca_percent or areac/arean>ca_percent: # large overlap, store the index\n",
    "                    paired_index.append([j, (areac/areat>ca_percent+areac/arean>ca_percent)/2])\n",
    "        if len(paired_index) > 0:\n",
    "            paired_index.sort(key=lambda ip: ip[1], reverse=True)\n",
    "            paired_index = paired_index[0][0]\n",
    "            label_pair.append([tlabel, labels[paired_index]])\n",
    "            paired_predicted.add(paired_index)\n",
    "        # give a fake label for image has ground true but model does not give a prediction\n",
    "        else:\n",
    "            label_pair.append([tlabel, 7])\n",
    "\n",
    "    unpaired_results = set(range(len(labels)))-paired_predicted\n",
    "    label_pair.extend([(7, labels[i]) for i in unpaired_results])\n",
    "    return label_pair\n",
    "\n",
    "\n",
    "def cal_metrics(test_dataset, test_epoch, score_filter=0.5, ca_percent=0.5, output_dir='t2ax_output'):\n",
    "    # load the detect results\n",
    "    test = torch.load('/path/to/%s/%s-%s.pth' % (output_dir, test_dataset, test_epoch))\n",
    "    t2 = 'T2_Ax' if test_dataset[:3] == 'int' else 'T2*Ax'\n",
    "    \n",
    "    label_pairs = []\n",
    "    for img_id, res in test.items():\n",
    "        ground_truth = load_ground_truth_test(img_id, test_dataset, t2)\n",
    "        label_pair = align_res_and_ground_truth(img_id, test_epoch, res, ground_truth, score_filter, ca_percent)\n",
    "        label_pairs.extend(label_pair)\n",
    " \n",
    "    df = pd.DataFrame(label_pairs, columns=['truth_label', 'predict_label'])\n",
    "    test_log = '/path/to/%s/test_log_%s.txt' % (output_dir, test_dataset)\n",
    "    with open(test_log, 'a') as fp:\n",
    "        acc = accuracy_score(list(zip(*label_pairs))[0], list(zip(*label_pairs))[1])\n",
    "        overall_recall = df[(df.truth_label<7)&(df.predict_label<7)].shape[0] / df[df.truth_label<7].shape[0]\n",
    "        fp.write('%s\\t%s\\tACC:%.4f\\tRecall:%.4f\\n' % (test_epoch, score_filter, acc, overall_recall))\n",
    "    for i in range(7):\n",
    "        label_count = df[df.truth_label==i].shape[0]\n",
    "        true_count = df[(df.truth_label==i)&(df.predict_label==i)].shape[0]\n",
    "        with open(test_log, 'a') as fp:\n",
    "            fp.write('%s\\t%4s/%4s\\t%.4f\\n' % (i, true_count, label_count, true_count/label_count*100))\n",
    "\n",
    "for test_epoch in range(9, 100, 10): \n",
    "    for score_filter in [0.1, 0.25, 0.5, 0.7, 0.9]:\n",
    "        cal_metrics('int-test-reference', test_epoch, score_filter, output_dir='t2ax_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150555b-e83a-4b3a-b601-ccca1f853f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_results(test_dataset, test_epoch, score_filter=0.5, ca_percent=0.5, output_dir='t2ax_output'):\n",
    "    test = torch.load('/path/to/%s/%s-%s.pth' % (output_dir, '%s-test-reference' % test_dataset[:3], test_epoch))\n",
    "    t2 = 'T2_Ax' if test_dataset[:3] == 'int' else 'T2*Ax'\n",
    "    \n",
    "    label_pairs = []\n",
    "    for img_id, res in test.items():\n",
    "        ground_truth = load_ground_truth_test(img_id, test_dataset, t2)\n",
    "        label_pair = align_res_and_ground_truth(img_id, test_epoch, res, ground_truth, score_filter, ca_percent)\n",
    "        label_pairs.extend(label_pair)\n",
    " \n",
    "    df = pd.DataFrame(label_pairs, columns=['truth_label', 'predict_label'])\n",
    "    \n",
    "    for i in range(7):\n",
    "        print(i, df[df.truth_label==i].shape[0])\n",
    "    \n",
    "    overall_recall = df[(df.truth_label<7)&(df.predict_label<7)].shape[0] / df[df.truth_label<7].shape[0]\n",
    "    print(\"Overall Recall:%.6f\" % overall_recall)\n",
    "    \n",
    "    df_central = df[df.truth_label<=3]\n",
    "    central_recall = df_central[df_central.predict_label<=3].shape[0] / df_central.shape[0]\n",
    "    print(\"Central Recall:%.6f\" % central_recall)\n",
    "    \n",
    "    df_side = df[(df.truth_label>3)&(df.truth_label<7)]\n",
    "    side_recall = df_side[(df_side.predict_label>3)&(df_side.predict_label<7)].shape[0] / df_side.shape[0]\n",
    "    print(\"Side Recall:%.6f\" % side_recall)\n",
    "    \n",
    "    # multi-class\n",
    "    df_overall = df[(df.truth_label<7)&(df.predict_label<7)]\n",
    "    overall_7_acc = accuracy_score(list(df_overall.truth_label), list(df_overall.predict_label))\n",
    "    print('7 Classes Acc:%.6f' % overall_7_acc)\n",
    "    \n",
    "    df_central = df[(df.truth_label<=3)&(df.predict_label<=3)]\n",
    "    central_4_acc = accuracy_score(list(df_central.truth_label), list(df_central.predict_label))\n",
    "    print('Central 4 Classes Acc:%.6f' % central_4_acc)\n",
    "    \n",
    "    df_side = df[((df.truth_label>3)&(df.truth_label<7))&((df.predict_label>3)&(df.predict_label<7))]\n",
    "    side_3_acc = accuracy_score(list(df_side.truth_label), list(df_side.predict_label))\n",
    "    print('Side 3 Classes Acc:%.6f' % side_3_acc)\n",
    "    \n",
    "    # binary class\n",
    "    cat_72_map = {0:0, 1:0, 2:1, 3:1, 4:2, 5:2, 6:3, 7:4}\n",
    "    pairs_72 = [(cat_72_map[p[0]], cat_72_map[p[1]]) for p in label_pairs]\n",
    "    df = pd.DataFrame(pairs_72, columns=['truth_label', 'predict_label'])\n",
    "    \n",
    "    df_2 = df[(df.truth_label<=3)&(df.predict_label<=3)]\n",
    "    overall_4_acc = accuracy_score(list(df_2.truth_label), list(df_2.predict_label))\n",
    "    print('Overall 4 Classes Acc:%.6f' % overall_4_acc)\n",
    "    \n",
    "    df_central = df[(df.truth_label<=1)&(df.predict_label<=1)]\n",
    "    central_2_acc = accuracy_score(list(df_central.truth_label), list(df_central.predict_label))\n",
    "    print('Central 2 Classes Acc:%.6f' % central_2_acc)\n",
    "    \n",
    "    df_side = df[((df.truth_label>1)&(df.truth_label<4))&((df.predict_label>1)&(df.predict_label<4))]\n",
    "    side_2_acc = accuracy_score(list(df_side.truth_label), list(df_side.predict_label))\n",
    "    print('Side 2 Classes Acc:%.6f' % side_2_acc)\n",
    "    \n",
    "cal_results('int-test-reference', 29, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b78447-2b54-469b-89f6-fe3d6160ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(test_dataset, test_epoch, score_filter=0.5, ca_percent=0.5, output_dir='t2ax_output'):\n",
    "    test = torch.load('/path/to/%s/%s-%s.pth' % (output_dir, '%s-test-reference' % test_dataset[:3], test_epoch))\n",
    "    t2 = 'T2_Ax' if test_dataset[:3] == 'int' else 'T2*Ax'\n",
    "    \n",
    "    label_pairs = []\n",
    "    for img_id, res in test.items():\n",
    "        if id_img_map[img_id].find('NUH257')>=0:\n",
    "            continue\n",
    "        ground_truth = load_ground_truth_test(img_id, test_dataset, t2)\n",
    "        label_pair = align_res_and_ground_truth(img_id, test_epoch, res, ground_truth, score_filter, ca_percent)\n",
    "        label_pairs.extend(label_pair)\n",
    "\n",
    "    df = pd.DataFrame(label_pairs, columns=['truth_label', 'predict_label'])\n",
    "        \n",
    "    # confusion matrix for central\n",
    "    # multi-class\n",
    "    c4_cm = {'t%s' % i: {'p%s' % j: df[(df.truth_label==i)&(df.predict_label==j)].shape[0] for j in [0, 1, 2, 3]} for i in [0, 1, 2, 3]}\n",
    "    print(pd.DataFrame(c4_cm))\n",
    "\n",
    "    # binary-class\n",
    "    c2_cm = {'t%s' % i: {'p%s' % j: df[(df.truth_label.isin(i))&(df.predict_label.isin(j))].shape[0] for j in [[0, 1], [2, 3]]} for i in [[0, 1], [2, 3]]}\n",
    "    print(pd.DataFrame(c2_cm))\n",
    "        \n",
    "    # for side\n",
    "    # multi-class\n",
    "    s3_cm = {'t%s' % i: {'p%s' % j: df[(df.truth_label==i)&(df.predict_label==j)].shape[0] for j in [4, 5, 6]} for i in [4, 5, 6]}\n",
    "    print(pd.DataFrame(s3_cm))\n",
    "\n",
    "    # binary-class\n",
    "    s2_cm = {'t%s' % i: {'p%s' % j: df[(df.truth_label.isin(i))&(df.predict_label.isin(j))].shape[0] for j in [[4, 5], [6]]} for i in [[4, 5], [6]]}\n",
    "    print(pd.DataFrame(s2_cm))\n",
    "\n",
    "    # summary info\n",
    "    print('all', df[df.truth_label<7].shape[0], df[df.predict_label<7].shape[0], df[(df.truth_label<7)&(df.predict_label<7)].shape[0])\n",
    "    for i in range(7):\n",
    "        print(i, df[df.truth_label==i].shape[0], df[df.predict_label==i].shape[0], df[(df.truth_label==i)&(df.predict_label==i)].shape[0])\n",
    "    for g in [[0, 1], [2, 3], [4, 5], [6]]:\n",
    "        print(g, df[df.truth_label.isin(g)].shape[0], df[df.predict_label.isin(g)].shape[0], df[(df.truth_label.isin(g))&(df.predict_label.isin(g))].shape[0])\n",
    "    \n",
    "confusion_matrix('int-test-reference', 29, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ee4c4-5afd-45f4-9d5c-dffc81d87a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
